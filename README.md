# Project_inj
# Лабораторное занятие №10–12  
### Анализ продаж Walmart с использованием PySpark + расширенная документация и динамический датакаталог

---

## 1. Краткое резюме проекта

**Цель проекта:**  
Изучить основы работы с PySpark и реализовать базовый ETL-конвейер для анализа продаж Walmart. Дополнительно — создать профессионально оформленную проектную документацию и реализовать динамический датакаталог для автоматического описания используемых данных.

**Задачи проекта:**
- загрузка исходных данных Walmart;
- первичная очистка и приведение типов;
- фильтрация и выделение значимых записей;
- расчёт основных статистических показателей;
- сохранение результатов в каталог `/output`;
- генерация динамического датакаталога с метаданными.

---

## 2. Архитектура данных и ETL-процесса

### Общая схема проекта
/data (сырьевые данные: Walmart_Sales.csv)
↓ Extract
/scripts (PySpark обработка)
↓ Transform
/output (результаты фильтрации и аналитики)
↓ Load
data_catalog.csv (динамический каталог данных)

### Этапы ETL

**Extract**  
- Загрузка `Walmart_Sales.csv`  
- Автоматическое определение типов PySpark  

**Transform**  
- Приведение `Weekly_Sales` к double  
- Фильтрация строк с продажами > 100  
- Сортировка по убыванию  
- Расчёт статистики  

**Load**  
- Сохранение результатов в `/output/high_amount_sales`  
- Формирование динамического каталога `data_catalog.csv`  

---

## 3. Используемые технологии
- **Python 3.11**
- **Apache Spark (PySpark)**
- **pandas**
- **os / pathlib** — работа с файлами
- **CSV** — формат входных данных

---

## 4. Структура каталогов проекта


Project_inj/
│
├── data/ # исходные данные (Walmart_Sales.csv)
├── output/ # результаты PySpark-фильтрации
│ └── high_amount_sales/
├── scripts/ # скрипты проекта
│ ├── analyse_walmart.py # основная PySpark обработка
│ └── generate_data_catalog.py # динамический датакаталог
├── notebooks/ # анализ, EDA
├── data_catalog.csv # автоматически обновляемый каталог данных
└── README.md # текущая документация
